{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b0e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b110a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "testin = [\"hello world!\",\"second input:\",\"third input:\"]\n",
    "testout = [\"hi!!!!!!!!\",\"yes youre second wow so great\",\"now finished\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/nova/cs336/assignment5-alignment/models/Qwen2.5-Math-1.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_tensor(io_len:list[tuple[int,int]])->torch.Tensor:\n",
    "    \"\"\"\n",
    "    根据已知的io长度制造mask，只显露出模型输出的部分，mask掉输入和尾部padding\n",
    "    \"\"\"\n",
    "    max_len = max([i + o for (i,o) in io_len])\n",
    "    res = []\n",
    "    for ilen, olen in io_len:\n",
    "        #print(\"i,o:\",ilen,olen)\n",
    "        imask = [0] * ilen\n",
    "        omask = [1] * olen\n",
    "        padmask = [0] * (max_len - ilen - olen)\n",
    "        res.append(imask + omask + padmask)\n",
    "    rest = torch.tensor(res)\n",
    "    return rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85186ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt_and_output(\n",
    "        prompt_strs:list[str],\n",
    "        output_strs:list[str], \n",
    "        tokenizer:PreTrainedTokenizerBase ):\n",
    "    \"\"\"\n",
    "    Tokenize the prompt and output strings, and construct a mask that is 1 for the response tokens and 0 for other tokens (prompt or padding).\n",
    "    Args:\n",
    "        prompt_strs(list[str]): List of prompt strings.\n",
    "        output_strs(list[str]): List of output strings.\n",
    "        tokenizer(PreTrainedTokenizer): Tokenizer to use for tokenization.\n",
    "    Returns:\n",
    "        output(dict[str, torch.Tensor]): Let prompt_and_output_lens be a list containing the lengths of the tokenized prompt and output strings. Then the returned dictionary should have the following keys.\n",
    "        - input_ids: torch.Tensor of shape (batch_size, max(prompt_and_output_lens) - 1): the tokenized prompt and output strings, with the final token sliced off.\n",
    "        - labels: torch.Tensor of shape (batch_size, max(prompt_and_output_lens) - 1): shifted input ids, i.e., the input ids without the first token.\n",
    "        - response_mask: torch.Tensor of shape (batch_size, max(prompt_and_output_lens) - 1): a mask on the response tokens in the labels.\n",
    "    \"\"\"\n",
    "    # print(\"输入:\",prompt_strs)\n",
    "    # print(\"输出:\",output_strs)\n",
    "\n",
    "    assert len(prompt_strs) == len(output_strs) , \"输入与输出数量不等！\"\n",
    "    \n",
    "    prompt_ids = [tokenizer.encode(s) for s in prompt_strs]\n",
    "    #print(prompt_ids)\n",
    "\n",
    "    response_ids = [tokenizer.encode(s) for s in output_strs]\n",
    "    #print(response_ids)\n",
    "    \n",
    "    batch_ids = [p + r for p,r in zip(prompt_ids, response_ids)]\n",
    "    #print(batch_ids)\n",
    "\n",
    "    io_len = [(len(lp),len(lr)) for lp, lr in zip(prompt_ids, response_ids)]\n",
    "    #print(io_len)\n",
    "\n",
    "    max_len = max([len(io) for io in batch_ids])\n",
    "    #print(\"最长的序列长度：\",max_len)\n",
    "\n",
    "    padding_id = tokenizer.pad_token_id\n",
    "    #print(\"padding id:\",padding_id)\n",
    "    #print(tokenizer.decode(padding_id))\n",
    "\n",
    "    for io in batch_ids:\n",
    "        l = len(io)\n",
    "        pad = [padding_id for _ in range(max_len - l)]\n",
    "        io += pad\n",
    "\n",
    "    #print(batch_ids)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    padded_batch_tensor = torch.tensor(batch_ids)\n",
    "    #print(padded_batch_tensor)\n",
    "\n",
    "    mask_tensor = get_mask_tensor(io_len)\n",
    "    #print(mask_tensor)\n",
    "\n",
    "    res =  {\n",
    "        \"input_ids\":padded_batch_tensor[:,:-1],\n",
    "        \"labels\":padded_batch_tensor[:,1:],\n",
    "        \"response_mask\":mask_tensor[:,1:]\n",
    "    }\n",
    "\n",
    "    return res\n",
    "\n",
    "tokenize_prompt_and_output(testin,testout,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8fc5a9",
   "metadata": {},
   "source": [
    "### Problem (compute_entropy): Per-token entropy (1 point)\n",
    "\n",
    "**Deliverable:** Implement a method `compute_entropy` that computes the per-token entropy of next-token predictions. The following interface is recommended:\n",
    "\n",
    "```python\n",
    "def compute_entropy(logits: torch.Tensor) -> torch.Tensor:\n",
    "```\n",
    "> Get the entropy of the next-token predictions (i.e., entropy over the vocabulary dimension).\n",
    "\n",
    "- **Args:**\n",
    "    - `logits: torch.Tensor`: Tensor of shape `(batch_size, sequence_length, vocab_size)` containing unnormalized logits.\n",
    "- **Returns:**\n",
    "    - `torch.Tensor`: Shape `(batch_size, sequence_length)`. The entropy for each next-token prediction.\n",
    "\n",
    "**Note:** you should use a numerically stable method (e.g., using `logsumexp`) to avoid overflow.\n",
    "\n",
    "To test your code, implement `adapters.run_compute_entropy`. Then run `uv run pytest -k test_compute_entropy` and ensure your implementation passes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from jaxtyping import Float, Int, Bool\n",
    "import torch\n",
    "\n",
    "def compute_entropy(\n",
    "    logits: Float[torch.Tensor,\"batch_size sequence_length vocab_size\"]\n",
    ") -> Float[torch.Tensor,\"batch_size sequence_length\"]:\n",
    "    \"\"\"\n",
    "    Get the entropy of the next-token predictions (i.e., entropy over the vocabulary dimension).\n",
    "    Args:\n",
    "        logits(torch.Tensor): Tensor of shape `(batch_size, sequence_length, vocab_size)` containing unnormalized logits.\n",
    "    Returns:\n",
    "        output(torch.Tensor): Shape `(batch_size, sequence_length)`. The entropy for each next-token prediction.\n",
    "    \"\"\"\n",
    "    # 每个位置取对数，乘以自己的相反数，然后沿最后一维求和\n",
    "    #print(\"输入：\",logits)\n",
    "    max_logits = einops.reduce(logits,\"b s v -> b s 1\",\"max\")   # b s 1\n",
    "    #print(max_logits)\n",
    "    logits -= max_logits                                        # b s v\n",
    "    \n",
    "    exp = torch.exp(logits)                                     # b s v\n",
    "    sumexp = einops.reduce(exp,\"b s v -> b s 1\",\"sum\")          # b s 1\n",
    "    prob = exp / sumexp                                         # b s v\n",
    "\n",
    "    logsumexp = torch.log(sumexp)                               # b s 1\n",
    "    logprob = logits - logsumexp                                # b s v\n",
    "\n",
    "    entropy_contrib = einops.einsum(-prob,logprob,\"b s v, b s v -> b s v\")\n",
    "    entropy = einops.reduce(entropy_contrib,\"b s v -> b s\",\"sum\")\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "a = torch.tensor([[[1,2],[3,4]],\n",
    "                  [[2,3],[4,5]]])\n",
    "compute_entropy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "- torch.log(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa10da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"/home/nova/cs336/assignment5-alignment/tests/_snapshots/test_compute_entropy.npz\")\n",
    "print(data.files)  \n",
    "\n",
    "for key in data.files:\n",
    "    print(f\"{key}: {data[key].shape}\")\n",
    "    print(data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4cbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "inputs = torch.randn(size=(2,10,100))\n",
    "compute_entropy(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(size=(1,2,3))\n",
    "print(test)\n",
    "compute_entropy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d54a0b",
   "metadata": {},
   "source": [
    "### Problem (get_response_log_probs): Response log-probs (and entropy) (2 points)\n",
    "\n",
    "**Deliverable:** Implement a method `get_response_log_probs` that gets per-token conditional log-probabilities (given the previous tokens) from a causal language model, and optionally the entropy of the model’s next-token distribution. The following interface is recommended:\n",
    "\n",
    "```python\n",
    "def get_response_log_probs(\n",
    "    model: PreTrainedModel,\n",
    "    input_ids: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    return_token_entropy: bool = False,\n",
    ") -> dict[str, torch.Tensor]:\n",
    "```\n",
    "\n",
    "- **Args:**\n",
    "    - `model: PreTrainedModel`: HuggingFace model used for scoring (placed on the correct device and in inference mode if gradients should not be computed).\n",
    "    - `input_ids: torch.Tensor`: shape `(batch_size, sequence_length)`, concatenated prompt + response tokens as produced by your tokenization method.\n",
    "    - `labels: torch.Tensor`: shape `(batch_size, sequence_length)`, labels as produced by your tokenization method.\n",
    "    - `return_token_entropy: bool`: If `True`, also return per-token entropy by calling `compute_entropy`.\n",
    "- **Returns:**\n",
    "    - `dict[str, torch.Tensor]`:\n",
    "        - `\"log_probs\"`: shape `(batch_size, sequence_length)`, conditional log-probabilities `log pθ(xt | x<t)`.\n",
    "        - `\"token_entropy\"`: optional, shape `(batch_size, sequence_length)`, per-token entropy for each position (present only if `return_token_entropy=True`).\n",
    "\n",
    "**Implementation tips:**\n",
    "- Obtain logits with `model(input_ids).logits`.\n",
    "\n",
    "To test your code, implement `adapters.run_get_response_log_probs`. Then run `uv run pytest -k test_get_response_log_probs` and ensure the test passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a64614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4c519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from cs336_alignment.mysft import *\n",
    "from transformers.models.auto.modeling_auto import AutoModelForCausalLM\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/nova/cs336/assignment5-alignment/models/Qwen2.5-Math-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/nova/cs336/assignment5-alignment/models/Qwen2.5-Math-1.5B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e6c349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[14582,    25,   220,    17,    10,    17, 19884, 21806,    25,   220]]), 'labels': tensor([[   25,   220,    17,    10,    17, 19884, 21806,    25,   220,    19]]), 'response_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompts = [\"Question: 2+2=? Answer:\"]\n",
    "responses = [\" 4\"]\n",
    "\n",
    "tok = tokenize_prompt_and_output(prompts, responses, tokenizer)\n",
    "input_ids = tok[\"input_ids\"]\n",
    "labels = tok[\"labels\"]\n",
    "\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "069aa9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 7.9035, 11.9540, 11.8372,  ..., -2.5852, -2.5849, -2.5850],\n",
      "         [ 2.5860,  4.3740,  3.6240,  ..., -4.1906, -4.1907, -4.1910],\n",
      "         [-1.2723, -1.3299,  0.0963,  ..., -2.0321, -2.0321, -2.0322],\n",
      "         ...,\n",
      "         [11.0863,  4.5606,  9.9343,  ..., -2.4296, -2.4293, -2.4298],\n",
      "         [ 1.5735,  0.9405,  0.1129,  ..., -4.3587, -4.3588, -4.3593],\n",
      "         [ 0.3099, -2.6036, -0.9576,  ..., -1.6830, -1.6830, -1.6833]]])\n",
      "torch.Size([1, 10, 151936])\n",
      "tensor([[   25,   220,    17,    10,    17, 19884, 21806,    25,   220,    19]])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "from cs336_alignment.mysft import *\n",
    "with torch.no_grad():\n",
    "    get_response_log_probs(model, input_ids, labels, return_token_entropy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.randint(0, 5, [2,3,4])\n",
    "print(P)\n",
    "I = torch.tensor([[1, 0, 2],\n",
    "                  [2, 2, 1]])\n",
    "\n",
    "R = torch.gather(P, dim=2, index=I.unsqueeze(-1)).squeeze(-1)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f879b96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "logits = torch.tensor([[[1,2,3]]])\n",
    "id = torch.tensor([[0]])\n",
    "prob = compute_prob_given_id(logits,id)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecaa6080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09003057317038046"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(math.e - 1)/(math.e ** 3 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3660276",
   "metadata": {},
   "source": [
    "### SFT microbatch train step\n",
    "\n",
    "The loss we minimize in SFT is the negative log-likelihood of the target output given the prompt. To compute this loss, we need to compute the log-probabilities of the target output given the prompt and sum over all tokens in the output, masking the tokens in the prompt and padding tokens.\n",
    "\n",
    "We will implement a helper function for this, that we will also make use of later during RL.\n",
    "\n",
    "### Problem (masked_normalize): Masked normalize (1 point)\n",
    "\n",
    "**Deliverable:** Implement a method `masked_normalize` that sums over tensor elements and normalizes by a constant while respecting a boolean mask. The following interface is recommended:\n",
    "\n",
    "```python\n",
    "def masked_normalize(\n",
    "    tensor: torch.Tensor,\n",
    "    mask: torch.Tensor,\n",
    "    normalize_constant: float,\n",
    "    dim: int | None = None,\n",
    ") -> torch.Tensor:\n",
    "```\n",
    "> Sum over a dimension and normalize by a constant, considering only those elements where `mask == 1`.\n",
    "\n",
    "- **Args:**\n",
    "    - `tensor: torch.Tensor`: The tensor to sum and normalize.\n",
    "    - `mask: torch.Tensor`: Same shape as `tensor`; positions with `1` are included in the sum.\n",
    "    - `normalize_constant: float`: the constant to divide by for normalization.\n",
    "    - `dim: int | None`: the dimension to sum along before normalization. If `None`, sum over all dimensions.\n",
    "- **Returns:**\n",
    "    - `torch.Tensor`: the normalized sum, where masked elements (`mask == 0`) don’t contribute to the sum.\n",
    "\n",
    "To test your code, implement `adapters.run_masked_normalize`. Then run `uv run pytest -k test_masked_normalize` and ensure it passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de6b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f039345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "tensor([[1, 0, 3],\n",
      "        [4, 5, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5., 3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cs336_alignment.mysft import *\n",
    "tensor = torch.tensor([[1,2,3],[4,5,6]])\n",
    "mask = torch.tensor([[1,0,1],[1,1,0]])\n",
    "c = 1.0\n",
    "dim = 0\n",
    "\n",
    "masked_normalize(tensor,mask,c,dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec399ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e338a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
